---
title: "Numerical Analysis"
date: 2024-05-17T14:20:55Z
draft: false
description: ""
tags: []
categories: []
disableComments: false
---

Numerical Analysis is a branch of mathematics that deals with the approximation of solutions to mathematical problems. It involves methods and algorithms for solving equations, systems of linear or nonlinear equations, interpolation, differentiation, integration, optimization
problems, and other mathematical problems. Numerical analysis is used in various fields such as engineering



**Lab1:** 
1) Stating the Problem
We want to study the behavior of the function
F(h) = (h - sin(h)) / (h^3)
near h = 0. Directly substituting h = 0 leads to a 0/0 form, so we suspect a limit exists but cannot just plug in h = 0. We also want to see how numerical round-off and cancellation errors affect computed results for very small h.

2) How to Solve the Problem (Overall Method)

Analytical Approach

Use a Taylor series expansion (or LHôpitals Rule) to find the limit as h approaches 0.
Determine the order of the error term in F(h) compared to its limiting value.
Numerical Approach and Plotting

Write a MATLAB code to evaluate F(h) for a range of h values, both large and small.
Plot F(h) in different scales (linear vs. log) to visualize how it behaves and to detect floating-point issues.
Compare the computed F(h) against the analytical limit L to see how the error scales with h.
By combining these steps, we can identify the theoretical limit and verify it with numerical experiments, while also illustrating how round-off errors arise at extremely small h.

3) Detailed Explanation of What Was Done

Define the Ranges of h

A logarithmically spaced range from 10^(-16) to 10^(1). This shows F(h) when h becomes very small or quite large.
A smaller linear range (for example, 0.01 to 1) to zoom in near h = 0 without going into extremely tiny values where floating-point precision might dominate.
Compute F(h) Numerically

In MATLAB, define vectors of h values for each range.
Compute (h - sin(h)) / (h^3) point-by-point.
Plot F(h) vs. h

Linear Scale Plot: Helpful to see broader behavior but can become “flat” at very small h.
Logarithmic Scale Plot (using semilogx): This makes it easier to see the behavior for small h and helps reveal when numerical errors start to dominate.
Observations: In the smallest h region (around 10^(-16)), subtractive cancellation (h - sin(h)) can cause F(h) to drop to zero or give spurious values.
Find the Analytical Limit L

From the Taylor series expansion of sin(h):
sin(h) = h - (h^3)/6 + higher-order terms.
Substituting into F(h):
F(h) = [h - (h - (h^3)/6 + ...)] / (h^3).
This simplifies to 1/6 + terms of order h^2.
Therefore, as h approaches 0, F(h) approaches 1/6.
Measure the Error and Determine the Order p

Define the absolute difference |F(h) - L| and plot it on a log10 vs. log10 scale against h.
When subtraction errors are not overwhelming, the slope of this plot is about 2, indicating the error term is on the order of h^2.
For extremely small h, cancellation in (h - sin(h)) can drastically distort results, causing a spike or flattening in the error curve.
Conclusions

The limit is 1/6.
The error behaves like O(h^2) as h approaches 0, consistent with the Taylor series.
Numerical pitfalls appear for h << 1, due to round-off and subtractive cancellation.
Combining analytic derivation and careful plotting confirms both the limit and the influence of floating-point arithmetic on F(h).
<br><br>
Report: 
![Alt text](/images/pics/MACM316/1.jpg "Optional Title")
<br><br><br><br>


**Lab2:**
1) Stating the Problem and Method

We have a family of large, sparse n-by-n matrices A_n generated by a discrete 1D Laplacian (tridiagonal with -2 on the main diagonal and 1 on each sub-/super-diagonal). Our goals are:

Part A: Measure how long it takes to compute the inverse of A_n using MATLAB’s inv() for different sizes n, and observe how the runtime scales.
Part B: Repeat a similar timing experiment but use LU factorization (lu()) instead, then compare how the runtime grows with n compared to Part A.
Part C: Solve the system A_n x = b in two ways (via inverse and via LU) and see whether the solutions match exactly.
We will plot the computing time on a log–log scale (log of matrix size versus log of average compute time) and use big-O ideas to describe how the time grows.

2) Detailed Explanation of How We Solved It

Part A (Inverse Timing)

Matrix Construction
For each target size n, build the sparse tridiagonal matrix A_n using spdiags([e, -2*e, e], -1:1, n, n), where e is a vector of ones.
Timing the Inverse
Within a loop, call inv(A_n) multiple times (here, 30 times) for each n.
Use tic and toc to measure elapsed time per call, then average these times.
Plot and Slope Extraction
Plot log(n) (the natural log of n) against log(average time).
Use a linear fit (via polyfit) on the “clean” region of the data to estimate the slope, which indicates how the runtime scales with n.
Part B (LU Factorization Timing)

Matrix Construction
As in Part A, generate A_n for a range of n (for example, from 2^8 to 2^17).
Timing the LU Decomposition
Again, loop over 30 runs of [L,U] = lu(A_n), measuring time with tic/toc, then average.
Plot and Compare
Plot the averaged times on a log–log scale, and fit a line to find the slope.
Compare to Part A’s slope to see which method grows faster as n increases.
Part C (Solving A_n x = b and Comparing x)

System Setup
Choose n = 100, form A_n, and define b as a simple vector (1, 2, 3, …, n).
Method 1 (Inverse)
Compute x1 = inv(A_n) * b.
Method 2 (LU Factorization)
Factor A_n into L and U.
Solve z = L \ b and then x2 = U \ z.
Comparison
Check if x1 and x2 are exactly the same by using isequal(x1, x2).
In practice, floating-point rounding often makes them differ slightly.
3) Key Findings

Part A: Inverting A_n directly (using inv()) typically shows a runtime that scales between O(n^2) and O(n^3). On the log–log plot, we observe a slope near 2 or slightly higher, indicating that computing the inverse of a sparse matrix is still quite expensive.
Part B: LU factorization of these tridiagonal matrices scales more efficiently, often close to O(n). In the log–log plot, we see a slope near 1–2, which is significantly less than the cost of a full inverse.
Part C:
The solutions x1 (from the inverse) and x2 (from LU) do not generally match bit-for-bit, owing to rounding and the different floating-point operations performed.
LU is typically the preferred numerical approach for solving A_n x = b, since computing the inverse is both more expensive and more prone to accumulated numerical error.
These experiments confirm that using LU decomposition is more efficient than forming a full inverse, especially for large sparse matrices, and also highlight the slight differences in computed solutions due to floating-point arithmetic.
<br><br>
Report: 
![Alt text](/images/pics/MACM316/2.jpg "Optional Title")
<br><br><br><br>


**Lab3:**
1) Stating the Problem and Methods

We aim to approximate 11^(1/6) starting from the initial guess p0 = 2 using three iterative methods:

Method 1
pn = p(n-1) - (p(n-1)^6 - 11) / [6 * p(n-1)^5]
(This corresponds to Newton’s method for x^6 - 11 = 0.)

Method 2
pn = p(n-1) - (p(n-1)^6 - 11) / 20

Method 3
pn = p(n-1) - (p(n-1)^6 - 11) / 24

We need to determine which methods converge or diverge and, among the convergent ones, which is fastest. We also estimate the order of convergence (alpha) and the asymptotic error constant (lambda).

2) How We Solved It

Implementation (Part A)

In MATLAB, each iteration updates p(n) using the respective formula.
We track the difference between successive iterates |p(n) - p(n-1)| until it is below a set tolerance or we exceed a maximum iteration count.
We compare the resulting approximate root to the actual value s = 11^(1/6).
Error Plots (Part B)

We run each method for a fixed number of iterations (for example, 30 to 100).
We compute the absolute error E(n) = |p(n) - s| at each iteration.
Plotting E(n) versus n shows which methods quickly decrease the error and which diverge.
Order and Error Constant (Part C)

For convergent methods, we assume E(n+1) is approximately lambda * [E(n)]^alpha.
Taking natural logs gives ln(E(n+1)) ~ ln(lambda) + alpha * ln(E(n)).
We fit a line to the data (using a linear regression of ln(E(n+1)) vs ln(E(n))) to estimate alpha (the slope) and ln(lambda) (the intercept).
Then lambda = exp(intercept).
3) Key Findings

Which Methods Converge

Method 1 (Newton’s) converges.
Method 2 diverges.
Method 3 converges but more slowly.
Speed of Convergence

Method 1 converges the fastest (approximately quadratic).
Method 3 converges more slowly (roughly linear).
Method 2 does not converge from p0 = 2.
Order and Error Constant

Method 1 has order alpha about 2, typical of Newton’s method, and a relatively small lambda, indicating rapid error reduction.
Method 3 has alpha near 1 (linear convergence) and a larger lambda, so it converges more gradually.
Overall, the experiments confirm that Newton’s method (Method 1) converges most rapidly, Method 2 diverges for this problem, and Method 3 exhibits slow but steady convergence.
<br><br>
Report: 
![Alt text](/images/pics/MACM316/3.jpg "Optional Title")
<br><br><br><br>


**Lab4:**
1) Stating the Problem and Method
We use natural (variational) cubic splines to approximate the function
f(x) = sin(x) – 1/3
over the interval [0, 1]. In Part A, we:

Construct the spline using the points x = 0, 0.25, 0.5, 0.75, and 1.0.
Identify the local cubic polynomial in the subinterval [0.25, 0.5].
Differentiate this polynomial to approximate the first and second derivatives at x = 0.25, then compare to the true values cos(0.25) and −sin(0.25).
In Part B, we examine the accuracy of those derivative approximations for progressively finer meshes, h = 2^(-m). We plot the absolute errors and use a log–log fit to determine the order p of the error as a function of h.

2) How We Carried Out the Analysis

Part A (Single Spline Construction)

Called csape(x, y, 'variational') in MATLAB to ensure natural boundary conditions.
Extracted the polynomial coefficients for the specific subinterval containing x = 0.25.
Differentiated that cubic polynomial manually to get formulas for f′(x) and f″(x). Evaluated both at x = 0.25.
Part B (Mesh Refinement Study)

Repeated the spline construction for a series of decreasing h values (h = 2^(-m), m from 2 to about 13).
For each spline, extracted the local polynomial around x = 0.25, computed f′(0.25) and f″(0.25), and calculated the absolute error compared to the exact derivatives.
Plotted these errors on log–log scales and used a linear regression (polyfit on log of h vs. log of the error) to estimate the slope p.
3) Key Findings

Part A:

The extracted spline polynomial on [0.25, 0.5] yields approximate derivatives f′(0.25) ≈ 0.9676 and f″(0.25) ≈ −0.2638, which are reasonably close to cos(0.25) and −sin(0.25).
Part B:

As h decreases, the errors in both first and second derivative approximations drop in a power‐law fashion.
The first derivative error behaves roughly like O(h^4.6), and the second derivative error behaves like O(h^2.0).
These orders are consistent with the theory that differentiating a cubic spline can give high‐order accuracy for the first derivative, with second derivative approximations typically being of lower order.
Overall, the experiment shows that natural cubic splines provide accurate derivative estimates, with first derivative errors often close to fourth‐order and second derivative errors around second‐order in h.
<br><br>
Report: 
![Alt text](/images/pics/MACM316/4.jpg "Optional Title")
<br><br><br><br>


**Lab5:**
1) Stating the Problem and Methods
We want to approximate the definite integral
∫ from 0 to 1 of (sin(x) – 1/3) dx
using two approaches:

Parts A and B: Composite Simpson’s Rule on uniform partitions (step sizes h = 2^(-m)).
Part C: A natural (variational) cubic spline built with the same node spacings, then integrated over [0, 1].
We compare both numerical approximations against the exact integral to see how the error scales with the step size h.

2) How We Carried Out the Analysis

Composite Simpson’s Rule (Parts A and B)

We wrote a function simpsons_rule that takes f(x), the interval [0,1], and step size h.
It applies Simpson’s formula with coefficients [1, 4, 2, 4, 2, …, 4, 1] and sums the discrete function values.
We reduce h in powers of two (for m = 2, 3, 4, 5), record the resulting errors, and plot log(error) against log(h).
A linear fit estimates the slope p, giving us O(h^p).
Natural Cubic Spline Integration (Part C)

For each h, we form a spline via csape(points, values, 'variational'), where points range from 0 to 1 with increments of h.
We integrate this spline (either piecewise or by a direct formula) and compare to the exact integral.
Again, we use log–log plots of error vs. h and fit a line to find the slope.
3) Key Findings

Composite Simpson’s Rule

Errors decrease roughly on the order of h^4.
A log–log fit shows a slope near 4, confirming fourth-order convergence with a smooth integrand.
Spline Integration

The errors for the spline-based integration behave like h^3.
A log–log slope near 3 indicates third-order convergence.
Comparison

For this function, composite Simpson’s Rule yields higher-order accuracy (p ≈ 4) than natural cubic spline integration (p ≈ 3).
Thus, Simpson’s Rule is more accurate here, demonstrating that a direct quadrature method can outperform spline interpolation when integrating a smooth function. 
<br><br>
Report: 
![Alt text](/images/pics/MACM316/5.jpg "Optional Title")
